\documentclass[main.tex]{subfiles}

\begin{document}
	
\section{Постановка задачи}
Поставлена проблема двумерной минимизации:
\begin{equation}\label{eq:problem}
f(x_1, x_2)=x_1^2+x_2^2+cos(x_1+3x_2)-x_1+2x_2 \rightarrow \min_{x_1, x_2}
\end{equation}
Необходимо:
\begin{itemize}
	\item Решить задачу (\ref{eq:problem}) методом наискорейшего спуска;
	\item Доказать сходимость метода наискорейшего спуска применительно к данной функции;
	\item Обосновать выбор условия окончания вычислений для метода одномерной минимизации по соответствующему условию градиентного метода, показать справедливость вывода в ходе вычислительного эксперимента при точности градиентного метода 0.01;
	\item Решить ту же задачу методом Ньютона второго порядка.
\end{itemize}

\section{Обоснование применимости методов}\label{section:proofs}
\subsection{Доказательство сходимости метода I порядка}
Оценим сходимость метода наискорейшего спуска, используя теорему 1.1 \cite{boldirev}. Покажем, что функция \ref{eq:problem} подчиняется следующим условиям:\\
\begin{gather} % amsmath package
\label{eq:convex} 
f(x) \in C^1(\mathds{R})\\
\label{eq:bounded}
\exists m \in \mathds{R}: m \le f(x) \forall x \in \mathds{R}^2 \\
\label{eq:lipshitz}
\exists L \in \mathds{R}: \norm{\nabla f(x) - \nabla f(y)} \le L\norm{x-y} \forall x,y \in \mathds{R}^2
\end{gather}

Для функции, удовлетворяющей вышеперечисленным условиям, в случае итерационной схемы градиентного спуска с параметром $eps > 0$, характеризующим окончание вычислений, теорема утверждает следующее: 
если номер шага $k\rightarrow \infty$, то выполняется условие выхода $\norm{\nabla f(x_k)}^2 \le eps$  (иначе говоря, при выполнении условий (\ref{eq:convex}), (\ref{eq:bounded}), (\ref{eq:lipshitz}) рано или поздно градиентный спуск -- в частности, метод наискорейшего спуска -- закончит работу).\\
Проверим выполнение условий.\\
\begin{gather*}
\frac{\partial f}{\partial x_1} = 2x_1 - \sin(x_1+3x_2)-1\\
\frac{\partial f}{\partial x_2} = 2x_2 - 3\sin(x_1+3x_2)+2
\end{gather*}
Частные производные по координатам непрерывны при любых $x_1, x_2$, значит, функция непрерывно дифференцируема в $\mathds{R}^2$ и (\ref{eq:convex}) выполнено.\\
Несложно убедиться, что (\ref{eq:bounded}) также выполнено. Перепишем минимизируемую функцию:
\begin{equation*}
\begin{aligned} % amsmath package
f(x_1, x_2) {}& = (x_1^2-x_1+\frac{1}{4})-\frac{1}{4}+(x_2^2+2x_2+1)-1 + cos(x_1+3x_2) = \\
 & = (x_1-\frac{1}{2})^2+(x_2+1)^2-1\frac{1}{4}+cos(x_1+3x_2)\ge -2\frac{1}{4}
\end{aligned}
\end{equation*}
Докажем существование константы Липшица $L$. Не умаляя общности, найдём $L$ для первой нормы $\norm{x}_1\overset{def}{=}|x_1|+|x_2|$, пользуясь эквивалентностью норм в $\mathds{R}^2$. Пусть
\begin{equation*}
x=\begin{pmatrix}x_1\\x_2\end{pmatrix}, y=\begin{pmatrix}y_1\\y_2\end{pmatrix} \in \mathds{R}^2 
\end{equation*}
Тогда
\begin{equation*}
\begin{aligned}
\norm{\nabla f(x) - \nabla f(y)} {}& = |2(x_1+x_2)-4\sin(x_1+3x_2) -2(y_1+y_2)+4\sin(y_1+3y_2)| \le \\
& \le 2|x_1+x_2-(y_1+y_2)|+4|\sin(x_1+3x_2)-\sin(y_1+3y_2)|  
\end{aligned}
\end{equation*}
Преобразуем трансцендентное слагаемое. Обозначим $a := x_1+3x_2, b := y_1+3y_2$ \\

\begin{gather*}
|\sin(a) - \sin(b)| = 2 \left|\sin \frac{a-b}{2} \cos \frac{a+b}{2}\right| \le 2 \left|\sin\frac{a-b}{2}\right|\le |a-b|=|x_1-y_1 + 3(x_2-y_2)|\\
\norm{\nabla f(x)-\nabla f(y)} \le 2 |(x_1+x_2)-(y_1+y_2)|+4|(x_1-y_1)+3(x_2-y_2)|\le \\
\le 4 |\Delta \vec{x}| + 16 |\Delta \vec{x}| = 20 |\Delta \vec{x}|  \Rightarrow \exists L = 20\\
\end{gather*}

\subsection{Обоснование применимости метода золотого сечения для нахождения шага}
\begin{figure}[h]
	\centering \includegraphics[width=\myPictWidth]{golden.jpg}
	\caption{Пример выбора длины начального интервала в методе золотого сечения}
	\label{im:golden}
\end{figure}
При поиске шага ставится задача: выйдя из точки $x_k$, найти в направлении антиградиента $-\nabla f(x_k)$ точку, дающую минимум функции $f(x_k-\alpha \nabla f(x_k))$. 
Пусть мы находимся в точке $x_k$ и выбираем шаг $\alpha_k$. Если $L$ -- константа Липшица градиента $\nabla f(x_k)$, то ближайший локальный минимум не может быть ближе к $x_k$, чем $\frac{\norm{\nabla f(x_k)}}{L}$. Также нерационально искать минимум слишком далеко от $x_k$, чтобы метод золотого сечения не занимал много времени. Можно искать $\Delta x \in \left[ \frac{\norm{f(x_k)}}{L}; 2\frac{\norm{f(x_k)}}{L}\right]$ -- в этом случае все возможные значения $f(x_{k+1})$ будут не больше, чем $f(x_k)$ (упрощённый пример приведён на рис. (\ref{im:golden}): исследуя на минимум $f(x)=-sin(x)$ и выходя из точки ноль, получаем $L=1$, $\norm{\nabla f(x)} = 1$ и интервал $[1;2]$), но на практике это может привести к ненужному измельчению шага. Авторы работы считают разумным поиск в диапазоне $\left[ \frac{\norm{f(x_k)}}{L}; 20\frac{\norm{f(x_k)}}{L}\right]$, т. е. $\alpha \in \left[\frac{1}{L}; \frac{20}{L}\right]$. \\

Длина желаемого интервала неопредённости, до которого нужно сузить начальный, должна быть такой, чтобыв итоге достичь условия \\

\begin{equation}\label{eq:stop-criteria}
\norm{\nabla f(x_k)} \le \epsilon
\end{equation}
 при $k \rightarrow \infty$. Для этого достаточно локализовать точку минимума $x_k^*$ одномерной функции с точностью $\frac{\varepsilon}{L}$. Такой выбор позволяет двигаясь в направлении точки с $\nabla f = 0$ гарантированно попасть в $x_k+1$ такую,что условие (\ref{eq:stop-criteria}) будет выполнено.


\section{Описание алгоритмов}
\subfile{gradient.tex}
\subsection{Метод Ньютона}
\includepdf[pages=-]{newton.pdf}
\section{Результаты решения задачи}
Оба алгоритма были запущены с начальным приближением $x_0=\begin{pmatrix}-5.0\\-1.5\end{pmatrix}$ и критерием остановки вычислений $\epsilon = 0.01$. Результаты проиллюстрированы графиками:
\begin{figure}[H]
	\centering \includegraphics[width=\myPictWidth]{newton.jpg}
	\caption{Вычисление минимума методом Ньютона 2 порядка}
	\label{im:newton}
\end{figure}
\begin{figure}[h]
	\centering \includegraphics[width=\myPictWidth]{steepest.jpg}
	\caption{Вычисление минимума методом наискорейшего спуска}
	\label{im:steepest}
\end{figure}

Алгоритм Ньютона сошёлся к точке $\begin{pmatrix}0.95\\0.35\end{pmatrix}$ с нормой $\norm{\nabla f} < \varepsilon$ за 18 шагов, а метод наискорейшего спуска к элементу $\begin{pmatrix}0.44\\-1.15\end{pmatrix}$ -- за 263 итерации. По рисункам видны некоторые особенности методов:
\begin{itemize}
	\item Алгоритм наискорейшего спуска, в отличие от метода Ньютона, не пропускает локальные минимумы (что согласуется с требованиями, которые предъялены к алгоритму поиска шага метода)
	\item В случае наискорейшего спуска при подходе к точке минимума шаги становятся реже (что обусловлено расширением диапазона, в котором выбираем $x_{k+1}$, при уменьшении нормы градиента), а в методе Ньютона -- чаще.
	\item Метод Ньютона выполнил меньше итераций, что согласуется с данными о более высокой скорости сходимости по сравнению с градиентными методами первого порядка \cite{boldirev}.
\end{itemize}

\section{Оценка достоверности результата}
Ранее в пункте \ref{section:proofs} было доказано, что алгоритм наискорейшего спуска всегда сходится к точке, где выполнено условие окончания итераций (\ref{eq:stop-criteria}). \\
Проведён дополнительный вычислительный эксперимент: алгоритм запущен несколько раз с разными начальными точками. Результаты сведены в таблицу.

\begin{table}[h]
	\label{table:results}
	\caption{Результаты поиска минимума методом наискорейшего спуска}
\begin{center}
	\begin{tabular} { | p{0.2\linewidth} | p{0.1\linewidth} | p{0.1\linewidth} | p{0.2\linewidth} | p{0.1\linewidth} |}
		\hline
стартовая точка& $x_{stop}^{(1)}$ & $x_{stop}^{(2)}$ & число итераций  &  $\nabla f(x_{stop})$   \\ \hline
		$[-5.0; -1.5]$ & $0.441$ & $-1.158$ & $263$  & $0.0098$  \\ \hline
		$[0.0; 0.0]$ & $0.441$ & $-1.158$ & $556$  & $0.0098$  \\ \hline
		$[-5.0; -5.0]$ & $0.451$ & $-1.162$ & $2844$  & $0.0095$  \\ \hline
		$[1.15; 0.29]$ & $0.956$ & $0.353$ & $77$  & $0.0098$  \\ \hline
		$[1.5; 0.5]$ & $0.956$ & $0.353$ & $261$  & $0.0095$  \\ \hline
	\end{tabular}
\end{center}
Видно, что алгоритм при разных начальных приближениях сходится к разным точкам оптимума, но всякий раз достигает точки, в которой выполнено условие (\ref{eq:stop-criteria}).
\end{table}


\begin{thebibliography}{99}
	\bibitem{boldirev} Методы оптимизации. Математическое программирование: учеб. пособие. / Ю.Я. Болдырев, Е.А. Родионова; С.-Петерб. гос. техн. ун-т. - СПб.: Изд-во СПбГТУ, 1999. -- 81 с.: ил.
	% \bibitem{karmanov} Карманов, В.Г. Математическое программирование: Учеб. пособие. -- 6-е изд., испр. -- М.: ФИЗМАТЛИТ, 2008. -- 264 с. % yet unused
	% \bibitem{galeev} Галеев, Э. М., Тихомиров, В. М. Оптимизация: теория, примеры, задачи. -- М.: Эдиториал УРСС, 2000. -- 320 с. % yet unused
\end{thebibliography}

\end{document}